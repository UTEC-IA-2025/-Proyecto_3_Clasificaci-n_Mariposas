{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1YoUozCdWgGMiWn5fprkENECgeltyZICo","timestamp":1755715069284},{"file_id":"1AuKojjAt3k3KNsZgL6ojEoZyL4qWQC3n","timestamp":1755713151185}],"authorship_tag":"ABX9TyNM7HfD/P7ACsAgiR3A1KDg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **EXPERIMENTO 07**"],"metadata":{"id":"UgO5fkvxJ-t-"}},{"cell_type":"markdown","source":["# **1. Preprocesamiento**\n","Para el preprocesamiento, se realizaron cuatro tareas básicas."],"metadata":{"id":"FsTuH0FY-BYs"}},{"cell_type":"markdown","source":["## **1.1 Carga y Organización del Dataset:**\n","El conjunto de datos, compuesto por imágenes de mariposas en formato **.png**, se cargó directamente desde el sistema de archivos, importandolo desde Kaggle (path = kagglehub.dataset_download(\"veeralakrishna/butterfly-dataset\").\n","Para organizar las imágenes, se creó un DataFrame de Pandas. La etiqueta (clase) de cada imagen se extrajo del nombre del archivo, que seguía el formato de [ID_categoria][número_secuencial].png. Este método permitió la categorización automatizada de las imágenes."],"metadata":{"id":"YkYpFkT9_xfj"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fgVgKduOpniE","executionInfo":{"status":"ok","timestamp":1755828872453,"user_tz":300,"elapsed":11741,"user":{"displayName":"Piero Andrés Díaz Carpio","userId":"17486678244017445526"}},"outputId":"f5a932f0-17f1-47bf-89ec-4edc06346d39"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"]}],"source":["!pip install pandas scikit-learn\n"]},{"cell_type":"code","source":["# Hacemos la importación de las librerías necesarias\n","import os\n","import kagglehub\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from PIL import Image\n","from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms"],"metadata":{"id":"V4IeDOHPtSCc","executionInfo":{"status":"ok","timestamp":1755828907032,"user_tz":300,"elapsed":15435,"user":{"displayName":"Piero Andrés Díaz Carpio","userId":"17486678244017445526"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Movemos el archivo a la carpeta correcta\n","!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json"],"metadata":{"id":"2anElS1Uvjjf","executionInfo":{"status":"ok","timestamp":1755829233570,"user_tz":300,"elapsed":308,"user":{"displayName":"Piero Andrés Díaz Carpio","userId":"17486678244017445526"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Descargamos la última versión\n","path = kagglehub.dataset_download(\"veeralakrishna/butterfly-dataset\")\n","\n","print(\"Ruta para los archivos del dataset:\", path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VB1JNDihv7vd","executionInfo":{"status":"ok","timestamp":1755829235529,"user_tz":300,"elapsed":1957,"user":{"displayName":"Piero Andrés Díaz Carpio","userId":"17486678244017445526"}},"outputId":"7da165fb-6062-4094-dd46-31d64dd5636b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Ruta para los archivos del dataset: /kaggle/input/butterfly-dataset\n"]}]},{"cell_type":"code","source":["# Definimos la ruta local\n","ruta_base = '/kaggle/input/butterfly-dataset/leedsbutterfly/'\n","ruta_imagenes = os.path.join(ruta_base, 'images')"],"metadata":{"id":"5-_fA8XPwwD2","executionInfo":{"status":"ok","timestamp":1755829235560,"user_tz":300,"elapsed":30,"user":{"displayName":"Piero Andrés Díaz Carpio","userId":"17486678244017445526"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Obtenemos todos los archivos en el directorio de imágenes\n","print(\"Contenido inicial del directorio de imágenes:\")\n","print(os.listdir(ruta_imagenes))\n","print(\"-\" * 50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W7fn6-SezAdE","executionInfo":{"status":"ok","timestamp":1755829235687,"user_tz":300,"elapsed":125,"user":{"displayName":"Piero Andrés Díaz Carpio","userId":"17486678244017445526"}},"outputId":"01fc567c-63fa-4fd1-ff88-f0459370baea"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Contenido inicial del directorio de imágenes:\n","['0020155.png', '0090177.png', '0090201.png', '0100038.png', '0070055.png', '0060059.png', '0070068.png', '0070044.png', '0090114.png', '0070099.png', '0050049.png', '0020112.png', '0100023.png', '0040082.png', '0100018.png', '0040034.png', '0060180.png', '0060030.png', '0050041.png', '0050007.png', '0050086.png', '0040057.png', '0030056.png', '0070059.png', '0050059.png', '0080030.png', '0010054.png', '0060185.png', '0020050.png', '0020092.png', '0060092.png', '0010078.png', '0050011.png', '0020186.png', '0050072.png', '0060188.png', '0090060.png', '0070027.png', '0060157.png', '0060186.png', '0060179.png', '0020144.png', '0100126.png', '0070003.png', '0100137.png', '0070016.png', '0020167.png', '0010017.png', '0070088.png', '0030057.png', '0050006.png', '0010063.png', '0090024.png', '0090116.png', '0090126.png', '0050050.png', '0070030.png', '0040015.png', '0050080.png', '0100050.png', '0020120.png', '0080059.png', '0080033.png', '0070102.png', '0040089.png', '0060120.png', '0020017.png', '0010086.png', '0040145.png', '0030051.png', '0090165.png', '0090006.png', '0030002.png', '0040045.png', '0070034.png', '0090001.png', '0090022.png', '0090118.png', '0040169.png', '0030053.png', '0090014.png', '0090065.png', '0020168.png', '0070107.png', '0040017.png', '0050043.png', '0090105.png', '0090123.png', '0040168.png', '0040121.png', '0020176.png', '0080018.png', '0030046.png', '0050013.png', '0050015.png', '0100145.png', '0060038.png', '0060206.png', '0010070.png', '0060097.png', '0070046.png', '0100005.png', '0050022.png', '0070047.png', '0100165.png', '0080055.png', '0090129.png', '0040031.png', '0070123.png', '0010029.png', '0030013.png', '0010022.png', '0030054.png', '0010028.png', '0040088.png', '0020141.png', '0050048.png', '0020105.png', '0040001.png', '0070114.png', '0060197.png', '0060142.png', '0070078.png', '0010083.png', '0060214.png', '0030062.png', '0070045.png', '0030007.png', '0090078.png', '0030009.png', '0040052.png', '0020081.png', '0010009.png', '0020181.png', '0010053.png', '0020063.png', '0080034.png', '0060049.png', '0060088.png', '0060217.png', '0020064.png', '0090157.png', '0050081.png', '0020033.png', '0080047.png', '0040069.png', '0090225.png', '0020174.png', '0070112.png', '0020040.png', '0100151.png', '0050054.png', '0010043.png', '0030008.png', '0030060.png', '0040073.png', '0070079.png', '0040070.png', '0090076.png', '0070057.png', '0070020.png', '0100006.png', '0040055.png', '0100128.png', '0040078.png', '0080035.png', '0040111.png', '0070006.png', '0100136.png', '0030059.png', '0090147.png', '0050037.png', '0090215.png', '0100154.png', '0090152.png', '0090219.png', '0090088.png', '0080025.png', '0060139.png', '0060198.png', '0060209.png', '0100003.png', '0090203.png', '0040099.png', '0080037.png', '0040134.png', '0060091.png', '0040051.png', '0010004.png', '0020135.png', '0050042.png', '0070017.png', '0070032.png', '0090119.png', '0020016.png', '0050058.png', '0060007.png', '0020052.png', '0010015.png', '0010005.png', '0010057.png', '0090087.png', '0050002.png', '0040019.png', '0030004.png', '0100107.png', '0020136.png', '0040138.png', '0060199.png', '0040036.png', '0100148.png', '0070011.png', '0100037.png', '0020044.png', '0100019.png', '0060046.png', '0060072.png', '0040141.png', '0080040.png', '0080046.png', '0010064.png', '0100150.png', '0050074.png', '0010066.png', '0090125.png', '0080009.png', '0020088.png', '0090115.png', '0050001.png', '0060150.png', '0100079.png', '0080003.png', '0070004.png', '0090221.png', '0080016.png', '0030012.png', '0070101.png', '0050004.png', '0040046.png', '0100147.png', '0040056.png', '0020074.png', '0070036.png', '0090054.png', '0030035.png', '0100160.png', '0050025.png', '0070081.png', '0040128.png', '0080041.png', '0060204.png', '0100143.png', '0080027.png', '0070072.png', '0100176.png', '0060087.png', '0100008.png', '0100058.png', '0020111.png', '0100025.png', '0040158.png', '0060147.png', '0020151.png', '0030047.png', '0050088.png', '0050079.png', '0020175.png', '0060050.png', '0090053.png', '0040074.png', '0080014.png', '0020055.png', '0090182.png', '0070103.png', '0050010.png', '0070022.png', '0060054.png', '0030022.png', '0100082.png', '0030021.png', '0020076.png', '0050091.png', '0080058.png', '0060080.png', '0030038.png', '0070115.png', '0070060.png', '0040071.png', '0090098.png', '0060145.png', '0070038.png', '0050016.png', '0100166.png', '0020125.png', '0020039.png', '0060105.png', '0060159.png', '0030033.png', '0020147.png', '0050069.png', '0060171.png', '0090099.png', '0080052.png', '0040030.png', '0090145.png', '0060118.png', '0020078.png', '0020128.png', '0050003.png', '0050014.png', '0050012.png', '0010074.png', '0090092.png', '0060074.png', '0060047.png', '0030026.png', '0040164.png', '0060123.png', '0050071.png', '0060034.png', '0100171.png', '0090191.png', '0040010.png', '0100039.png', '0050021.png', '0060025.png', '0040124.png', '0030016.png', '0060224.png', '0060107.png', '0060009.png', '0100041.png', '0100132.png', '0060086.png', '0040058.png', '0060093.png', '0050060.png', '0050027.png', '0010075.png', '0060154.png', '0100096.png', '0090162.png', '0010031.png', '0080002.png', '0020020.png', '0090032.png', '0050061.png', '0070086.png', '0010049.png', '0020046.png', '0010084.png', '0040068.png', '0060138.png', '0010036.png', '0090049.png', '0010032.png', '0010021.png', '0050046.png', '0050084.png', '0100157.png', '0030015.png', '0080010.png', '0050066.png', '0090143.png', '0070085.png', '0060175.png', '0030034.png', '0020154.png', '0050045.png', '0070010.png', '0100022.png', '0040142.png', '0100163.png', '0070100.png', '0050018.png', '0060228.png', '0060026.png', '0080028.png', '0060043.png', '0020058.png', '0060177.png', '0100086.png', '0090028.png', '0070121.png', '0100089.png', '0040144.png', '0010061.png', '0030001.png', '0010055.png', '0070098.png', '0070012.png', '0090226.png', '0040155.png', '0030039.png', '0040022.png', '0090039.png', '0020030.png', '0010013.png', '0030028.png', '0010046.png', '0080045.png', '0070029.png', '0060098.png', '0010025.png', '0020042.png', '0040018.png', '0010041.png', '0050039.png', '0010008.png', '0020026.png', '0060151.png', '0030030.png', '0010038.png', '0010044.png', '0020071.png', '0020117.png', '0020008.png', '0050052.png', '0100092.png', '0010062.png', '0100054.png', '0060045.png', '0010024.png', '0070043.png', '0050056.png', '0010071.png', '0070069.png', '0020034.png', '0090156.png', '0070056.png', '0010047.png', '0010085.png', '0070065.png', '0060131.png', '0030055.png', '0090044.png', '0030036.png', '0020024.png', '0070033.png', '0010048.png', '0100124.png', '0100029.png', '0050047.png', '0040143.png', '0090180.png', '0020070.png', '0020169.png', '0060029.png', '0100162.png', '0040038.png', '0030032.png', '0020098.png', '0050029.png', '0100149.png', '0040009.png', '0030052.png', '0030027.png', '0020082.png', '0040130.png', '0010077.png', '0040114.png', '0020146.png', '0090178.png', '0070005.png', '0090100.png', '0100020.png', '0080043.png', '0100091.png', '0020096.png', '0020138.png', '0030005.png', '0020164.png', '0040133.png', '0040170.png', '0050008.png', '0090132.png', '0070076.png', '0010045.png', '0100172.png', '0030018.png', '0010058.png', '0100155.png', '0020143.png', '0090184.png', '0010067.png', '0040087.png', '0070084.png', '0060193.png', '0040064.png', '0070094.png', '0090136.png', '0080006.png', '0020160.png', '0090212.png', '0090010.png', '0100175.png', '0070001.png', '0020054.png', '0090130.png', '0010082.png', '0080005.png', '0070092.png', '0020015.png', '0010033.png', '0040113.png', '0020004.png', '0020115.png', '0020159.png', '0050062.png', '0080054.png', '0080013.png', '0100122.png', '0080053.png', '0040161.png', '0050044.png', '0060117.png', '0090061.png', '0010076.png', '0010034.png', '0040104.png', '0100123.png', '0010011.png', '0020041.png', '0090086.png', '0040102.png', '0060115.png', '0090027.png', '0020060.png', '0090011.png', '0030003.png', '0050082.png', '0040120.png', '0060183.png', '0070105.png', '0040035.png', '0100031.png', '0060218.png', '0050087.png', '0010002.png', '0040061.png', '0050031.png', '0070080.png', '0030040.png', '0040095.png', '0010056.png', '0080031.png', '0100067.png', '0070014.png', '0010042.png', '0050078.png', '0010059.png', '0050036.png', '0060136.png', '0090144.png', '0060056.png', '0040013.png', '0060041.png', '0070110.png', '0050067.png', '0030025.png', '0050089.png', '0040105.png', '0070008.png', '0010006.png', '0010052.png', '0100009.png', '0070083.png', '0020145.png', '0100130.png', '0090096.png', '0010065.png', '0100113.png', '0070111.png', '0040047.png', '0040002.png', '0100036.png', '0020027.png', '0090223.png', '0050065.png', '0040136.png', '0090084.png', '0030049.png', '0060100.png', '0080017.png', '0080048.png', '0100042.png', '0010068.png', '0050028.png', '0100045.png', '0070108.png', '0090175.png', '0090103.png', '0070082.png', '0050090.png', '0010019.png', '0060152.png', '0060023.png', '0090033.png', '0070106.png', '0070075.png', '0050019.png', '0040146.png', '0010069.png', '0040048.png', '0080029.png', '0080042.png', '0050024.png', '0040159.png', '0070064.png', '0100072.png', '0040127.png', '0070052.png', '0030024.png', '0010037.png', '0080036.png', '0070096.png', '0030050.png', '0050063.png', '0040049.png', '0050076.png', '0100028.png', '0010027.png', '0070026.png', '0080008.png', '0050038.png', '0020072.png', '0020104.png', '0090176.png', '0080012.png', '0020028.png', '0050057.png', '0060113.png', '0020095.png', '0100120.png', '0060146.png', '0010079.png', '0090067.png', '0010035.png', '0060106.png', '0030041.png', '0050051.png', '0090117.png', '0070019.png', '0040003.png', '0100115.png', '0070063.png', '0030043.png', '0090135.png', '0040037.png', '0070066.png', '0020021.png', '0040042.png', '0070104.png', '0010007.png', '0080024.png', '0020161.png', '0060084.png', '0010072.png', '0080038.png', '0050073.png', '0100173.png', '0080020.png', '0090146.png', '0060128.png', '0070024.png', '0070062.png', '0100099.png', '0020087.png', '0100133.png', '0020031.png', '0030023.png', '0100047.png', '0050009.png', '0040066.png', '0090069.png', '0040119.png', '0020037.png', '0060094.png', '0100134.png', '0070124.png', '0070097.png', '0100097.png', '0010039.png', '0040149.png', '0050020.png', '0020110.png', '0020073.png', '0030006.png', '0030058.png', '0050034.png', '0010050.png', '0090097.png', '0060129.png', '0090013.png', '0050035.png', '0020133.png', '0100110.png', '0100119.png', '0090041.png', '0070117.png', '0010012.png', '0100111.png', '0080004.png', '0030020.png', '0060164.png', '0030029.png', '0010051.png', '0090107.png', '0040098.png', '0020061.png', '0100030.png', '0050077.png', '0090109.png', '0050005.png', '0020121.png', '0020085.png', '0070053.png', '0060166.png', '0060160.png', '0030031.png', '0060039.png', '0010001.png', '0030017.png', '0050083.png', '0030044.png', '0080044.png', '0090122.png', '0060172.png', '0080007.png', '0090188.png', '0030042.png', '0070061.png', '0010073.png', '0030014.png', '0080051.png', '0090038.png', '0060077.png', '0090160.png', '0090204.png', '0010020.png', '0100044.png', '0060021.png', '0100093.png', '0010030.png', '0020101.png', '0020057.png', '0100034.png', '0080022.png', '0030037.png', '0100051.png', '0050023.png', '0040024.png', '0060037.png', '0040065.png', '0040044.png', '0070037.png', '0040157.png', '0020003.png', '0100077.png', '0060108.png', '0080049.png', '0060036.png', '0050070.png', '0010040.png', '0060048.png', '0050068.png', '0090127.png', '0010010.png', '0070074.png', '0020139.png', '0060195.png', '0060063.png', '0060153.png', '0020023.png', '0080026.png', '0080001.png', '0010023.png', '0070025.png', '0010026.png', '0040062.png', '0080039.png', '0060127.png', '0050040.png', '0060163.png', '0030045.png', '0020119.png', '0070050.png', '0040110.png', '0100016.png', '0060200.png', '0030061.png', '0040122.png', '0100024.png', '0100012.png', '0080032.png', '0010018.png', '0010060.png', '0080057.png', '0050032.png', '0030010.png', '0040033.png', '0030011.png', '0050085.png', '0050026.png', '0050064.png', '0050030.png', '0080019.png', '0080021.png', '0060125.png', '0050075.png', '0090110.png', '0090134.png', '0080050.png', '0070039.png', '0020142.png', '0040117.png', '0100153.png', '0020153.png', '0060212.png', '0060194.png', '0090140.png', '0040152.png', '0030048.png', '0050055.png', '0020171.png', '0010014.png', '0090062.png', '0020091.png', '0070007.png', '0040084.png', '0070093.png', '0060126.png']\n","--------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["## **1.2 Filtrado de archivos**\n","Para asegurar la integridad del dataset y evitar errores, se implementó un filtro. Se procesaron únicamente los archivos que terminaban con la extensión **.png**, **descartando cualquier otro tipo de archivo** o directorio que no correspondiera a una imagen del dataset."],"metadata":{"id":"XG4LGZEaATGf"}},{"cell_type":"code","source":["lista_archivos_filtrada = [f for f in os.listdir(ruta_imagenes) if f.endswith('.png')]"],"metadata":{"id":"NUzPmsPP1eXc","executionInfo":{"status":"ok","timestamp":1755829235691,"user_tz":300,"elapsed":5,"user":{"displayName":"Piero Andrés Díaz Carpio","userId":"17486678244017445526"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Aquí imprimimos la lista de archivos que realmente se van a procesar\n","print(\"Archivos filtrados para el procesamiento:\")\n","# Imprimimos solo los primeros 10\n","print(lista_archivos_filtrada[:10])\n","print(f\"\\nTotal de archivos a procesar: {len(lista_archivos_filtrada)}\")\n","print(\"-\" * 50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HlyhFvmI1h_j","executionInfo":{"status":"ok","timestamp":1755829235780,"user_tz":300,"elapsed":87,"user":{"displayName":"Piero Andrés Díaz Carpio","userId":"17486678244017445526"}},"outputId":"65f084f6-4836-4d05-9bed-c612617c28e8"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Archivos filtrados para el procesamiento:\n","['0020155.png', '0090177.png', '0090201.png', '0100038.png', '0070055.png', '0060059.png', '0070068.png', '0070044.png', '0090114.png', '0070099.png']\n","\n","Total de archivos a procesar: 832\n","--------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["## **1.3 División del Dataset**\n","El DataFrame se dividió en tres subconjuntos disjuntos: entrenamiento (80%), validación (10%) y prueba (10%). La división se realizó de manera estratificada (stratify), garantizando que la proporción de imágenes de cada clase de mariposa fuera la misma en los tres conjuntos. Esto es crucial para que el modelo se entrene y evalúe con una distribución de clases representativa y balanceada, evitando sesgos."],"metadata":{"id":"xz-IX_tVAsfa"}},{"cell_type":"code","source":["if not lista_archivos_filtrada:\n","    print(\"Error: No se encontraron archivos de imágenes con formato .png\")\n","else:\n","    # Creamos el DataFrame con los archivos válidos\n","    df_imagenes = pd.DataFrame({'filepath': [os.path.join(ruta_imagenes, f) for f in lista_archivos_filtrada]})\n","\n","    # Extraemos la etiqueta tomando los primeros 3 caracteres\n","    df_imagenes['label'] = df_imagenes['filepath'].apply(lambda x: os.path.basename(x)[:3])\n","\n","    # Verificamos que se creó el DataFrame correctamente\n","    print(\"Primeras 5 filas del DataFrame con las etiquetas extraídas:\")\n","    print(df_imagenes.head())\n","    print(\"-\" * 50)\n","\n","    # Imprimimos el conteo de imágenes por clase para verificar que todo está bien\n","    print(\"Conteo de imágenes por clase:\")\n","    print(df_imagenes['label'].value_counts())\n","    print(\"-\" * 50)\n","\n","    # Aquí dividimos los datos en conjuntos de entrenamiento, validación y prueba\n","    train_df, temp_df = train_test_split(df_imagenes, test_size=0.2, random_state=42, stratify=df_imagenes['label'])\n","    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n","\n","    # Hacemos una verificación final\n","    print(\"Verificación de los tamaños de los conjuntos de datos:\")\n","    print(f\"Número de imágenes para entrenamiento: {len(train_df)}\")\n","    print(f\"Número de imágenes para validación: {len(val_df)}\")\n","    print(f\"Número de imágenes para prueba: {len(test_df)}\")\n","    print(\"-\" * 50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u3H34cUr76dd","executionInfo":{"status":"ok","timestamp":1755829235789,"user_tz":300,"elapsed":13,"user":{"displayName":"Piero Andrés Díaz Carpio","userId":"17486678244017445526"}},"outputId":"3bba897f-0b56-4030-d769-6c8774d055bd"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Primeras 5 filas del DataFrame con las etiquetas extraídas:\n","                                            filepath label\n","0  /kaggle/input/butterfly-dataset/leedsbutterfly...   002\n","1  /kaggle/input/butterfly-dataset/leedsbutterfly...   009\n","2  /kaggle/input/butterfly-dataset/leedsbutterfly...   009\n","3  /kaggle/input/butterfly-dataset/leedsbutterfly...   010\n","4  /kaggle/input/butterfly-dataset/leedsbutterfly...   007\n","--------------------------------------------------\n","Conteo de imágenes por clase:\n","label\n","006    100\n","002     93\n","009     90\n","004     90\n","007     89\n","005     88\n","010     84\n","001     82\n","003     61\n","008     55\n","Name: count, dtype: int64\n","--------------------------------------------------\n","Verificación de los tamaños de los conjuntos de datos:\n","Número de imágenes para entrenamiento: 665\n","Número de imágenes para validación: 83\n","Número de imágenes para prueba: 84\n","--------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["## **1.4 Normalización**\n","En esta etapa ajustaremos los valores de los píxeles de las imágenes para que se encuentren en un rango estándar, en este caso será al rango **[0,1]**.  Los modelos de aprendizaje profundo, como el **Multilayer Perceptron (MLP)** que usarán, se entrenan más rápido y de manera más estable cuando los datos de entrada están en un rango pequeño y consistente. Las imágenes tienen valores de píxeles que van de **0 a 255**, que es un rango bastante grande. Reducir esto ayuda al modelo a aprender de forma más eficiente.\n","\n","Para esta etapa, necesitamos usar librerías de procesamiento de imágenes. Las más comunes son PIL (Pillow) o OpenCV. En este proyecto, como el siguiente paso es usar **PyTorch**, el método más conveniente es usar las transformaciones de la librería **torchvision**.\n","\n","La intención es cargar cada imagen, convertirla en un tensor de PyTorch, normalizar los valores de los píxeles, redimensionar la imagen a un tamaño fijo, que podría ser 64x64 píxeles, ya que las entradas de la red neuronal deben tener el mismo tamaño."],"metadata":{"id":"fClQaiPd-KGB"}},{"cell_type":"code","source":[],"metadata":{"id":"qqFXQQfkEXEN","executionInfo":{"status":"ok","timestamp":1755829235790,"user_tz":300,"elapsed":4,"user":{"displayName":"Piero Andrés Díaz Carpio","userId":"17486678244017445526"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Definimos las transformaciones para el conjunto de entrenamiento, cabe señalar que el incremento de datos solo se aplica al entrenamiento\n","transformaciones_entrenamiento = transforms.Compose([\n","    transforms.Resize((64, 64)),\n","    # Volteamos la imagen horizontalmente\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    # Rotamos la imagen en un ángulo aleatorio\n","    transforms.RandomRotation(degrees=15),\n","    transforms.ToTensor()\n","])\n","\n","# Definimos las transformaciones para los conjuntos de validación y prueba, no se usa aumentación en estos conjuntos\n","transformaciones_validacion_prueba = transforms.Compose([\n","    # Redimensionamos las imágenes a un tamaño fijo.\n","    transforms.Resize((64, 64)),\n","    # Convertimos la imagen a un tensor de PyTorch.\n","    transforms.ToTensor(),\n","])"],"metadata":{"id":"mURD77NRESJh","executionInfo":{"status":"ok","timestamp":1755829235791,"user_tz":300,"elapsed":4,"user":{"displayName":"Piero Andrés Díaz Carpio","userId":"17486678244017445526"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Para verificación, hace el uso de una sola imagen:\n","ruta_de_ejemplo = train_df['filepath'].iloc[0]\n","\n","# Abrimos la imagen usando la librería PIL (Pillow)\n","imagen_original = Image.open(ruta_de_ejemplo)\n","\n","# Aplicamos la cadena de transformaciones a la imagen.\n","imagen_normalizada = transformaciones_validacion_prueba(imagen_original)\n","\n","# Verificamos la forma del tensor (Canales x Alto x Ancho)\n","print(f\"Forma del tensor normalizado: {imagen_normalizada.shape}\")\n","\n","# Imprimimos una muestra de los valores de píxeles\n","print(f\"Valores de píxeles normalizados (muestra): \\n{imagen_normalizada[0, 0, :5]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ou6tqSp5-Ok1","executionInfo":{"status":"ok","timestamp":1755829235854,"user_tz":300,"elapsed":31,"user":{"displayName":"Piero Andrés Díaz Carpio","userId":"17486678244017445526"}},"outputId":"e8e6d406-5780-4523-a4f6-4e77c86cd9fd"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Forma del tensor normalizado: torch.Size([3, 64, 64])\n","Valores de píxeles normalizados (muestra): \n","tensor([0.1725, 0.1804, 0.1686, 0.1686, 0.1412])\n"]}]},{"cell_type":"markdown","source":["El resultado **torch.Size([3, 64, 64])** es muy importante. Nos dice que, para el ejemplo de la primera imagen, el tensor tiene tres dimensiones:\n","\n","*   **3:** Esto representa los canales de color (Rojo, Verde y Azul - RGB).\n","\n","*   **64:** Esto es la altura de la imagen en píxeles.\n","\n","*   **64:** Esto es la anchura de la imagen en píxeles.\n","\n","El orden de las dimensiones es [canales, altura, anchura], que es el formato estándar de PyTorch para trabajar con imágenes. La segunda parte del resultado, **tensor([0.1725, 0.1804, 0.1686, 0.1686, 0.1412])**, muestra que los valores de los píxeles ya no están en el rango de 0 a 255, sino de 0 a 1. Esto confirma que la normalización se ha realizado correctamente."],"metadata":{"id":"xwaqccHOFgez"}},{"cell_type":"markdown","source":["Ahora que sabes que la normalización funciona para una sola imagen, el siguiente paso es **aplicarla a todo el conjunto de datos de entrenamiento**, validación y prueba. Esto se hace con una herramienta del mismo PyTorch llamada DataLoader que automatiza este proceso. Como se ve acontinuación, **se crean los DataLoaders para cada uno de tus conjuntos de datos** (train_df, val_df, y test_df)."],"metadata":{"id":"FVeYi8fqFYj7"}},{"cell_type":"markdown","source":["### **1.4.1 Crear una Clase de Dataset Personalizada**"],"metadata":{"id":"2Y-k5EfvHLW9"}},{"cell_type":"code","source":["# Definimos la clase personalizada del dataset\n","class ButterflyDataset(Dataset):\n","    def __init__(self, dataframe, transform=None):\n","        self.dataframe = dataframe\n","        self.transform = transform\n","        self.clases = sorted(list(self.dataframe['label'].unique()))\n","        self.clase_a_indice = {clase: i for i, clase in enumerate(self.clases)}\n","\n","    def __len__(self):\n","        # Esto entrega el número total de imágenes en el dataset\n","        return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        # Obtenemos la ruta y la etiqueta de la fila actual\n","        ruta_imagen = self.dataframe.iloc[idx]['filepath']\n","        etiqueta_str = self.dataframe.iloc[idx]['label']\n","\n","        # Cargamos la imagen desde la ruta\n","        imagen = Image.open(ruta_imagen)\n","\n","        # Convertimos la etiqueta de texto (ej. '001') a un índice numérico\n","        etiqueta_idx = self.clase_a_indice[etiqueta_str]\n","\n","        # Aplicamos la transformación a la imagen\n","        if self.transform:\n","            imagen = self.transform(imagen)\n","\n","        # Aqui devolvemos la imagen como un tensor y la etiqueta como un tensor de tipo long\n","        return imagen, torch.tensor(etiqueta_idx, dtype=torch.long)"],"metadata":{"id":"_wmq7lrQHDoB","executionInfo":{"status":"ok","timestamp":1755829236127,"user_tz":300,"elapsed":272,"user":{"displayName":"Piero Andrés Díaz Carpio","userId":"17486678244017445526"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["### **1.4.2 Crear los DataLoaders**"],"metadata":{"id":"eVfTE-KbIL5H"}},{"cell_type":"code","source":["# Creamos las instancias del dataset con las transformaciones correctas\n","dataset_train = ButterflyDataset(train_df, transform=transformaciones_entrenamiento)\n","dataset_val = ButterflyDataset(val_df, transform=transformaciones_validacion_prueba)\n","dataset_test = ButterflyDataset(test_df, transform=transformaciones_validacion_prueba)\n","\n","# Definimos el tamaño del lote\n","batch_size = 32\n","\n","# Creamos los DataLoaders\n","dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n","dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n","dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"],"metadata":{"id":"1tJCaem6Gru5","executionInfo":{"status":"ok","timestamp":1755829236130,"user_tz":300,"elapsed":6,"user":{"displayName":"Piero Andrés Díaz Carpio","userId":"17486678244017445526"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# **2. Creación del Modelo MLP**\n","\n","Aquí tendremos una Capa de Entrada (Input Layer) que recibirá el tensor de la imagen. Luego pasará por las Capas Ocultas (Hidden Layers) donde las neuronas procesan la información de la capa anterior, aplicando cálculos y activaciones. Cuantas más capas ocultas y más neuronas, más compleja se hace la red. Por último, tendremos una Capa de Salida (Output Layer), la cual genera la predicción final. Para nuestra tarea de clasificación de 10 mariposas, esta capa tendrá 10 neuronas, una para cada tipo de mariposa. A continuación definiremos esta estructura en PyTorch. Usaremos la clase **torch.nn.Module**, que es el pilar para construir redes neuronales en **PyTorch**."],"metadata":{"id":"Q7AAF_0xLQnE"}},{"cell_type":"code","source":["dropout_rate = 0.0\n","\n","class MLP(nn.Module):\n","    def __init__(self, input_size, hidden_layers, num_classes, dropout_rate=dropout_rate):\n","        super(MLP, self).__init__()\n","\n","        # Definimos la arquitectura de la red en el constructor\n","        self.layers = nn.ModuleList()\n","\n","        # Esta es la capa de entrada\n","        self.layers.append(nn.Linear(input_size, hidden_layers[0]))\n","\n","        # Estas son las capas ocultas\n","        for i in range(len(hidden_layers) - 1):\n","            self.layers.append(nn.Linear(hidden_layers[i], hidden_layers[i+1]))\n","\n","        # Se añade la capa de dropout después de cada capa lineal (excepto la de salida)\n","        self.dropout = nn.Dropout(p=dropout_rate)\n","\n","        # Creamos la capa de salida como un módulo independiente\n","        self.output_layer = nn.Linear(hidden_layers[-1], num_classes)\n","\n","    def forward(self, x):\n","        # Aplanamos el tensor de la imagen de 3D a 1D, lo cual es necesario porque el MLP procesa datos planos, no imágenes 3D\n","        x = x.view(x.size(0), -1)\n","\n","        # Pasamos los datos a través de cada capa, aplicando la función de activación ReLU\n","        for layer in self.layers:\n","            x = F.relu(layer(x))\n","            # Aplicamos dropout después de la activación\n","            x = self.dropout(x)\n","\n","        # Pasamos los datos a través de la última capa sin activación, y la función de pérdida 'CrossEntropyLoss' de PyTorch ya incluye la función softmax\n","        x = self.output_layer(x)\n","        return x"],"metadata":{"id":"VuK7MLo_OPnL","executionInfo":{"status":"ok","timestamp":1755829236131,"user_tz":300,"elapsed":5,"user":{"displayName":"Piero Andrés Díaz Carpio","userId":"17486678244017445526"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["**__init__:** En este método, definimos las capas de la red. nn.Linear crea una capa totalmente conectada, lo que significa que cada neurona en una capa está conectada a cada neurona en la siguiente. nn.ModuleList es una lista para almacenar estas capas de manera que PyTorch las reconozca como parte del modelo.\n","\n","**forward:** Este método es la parte más importante. Define cómo fluyen los datos (el tensor x) desde la entrada hasta la salida.\n","\n","**x.view(x.size(0), -1):** Este comando aplana la imagen. Un tensor de imagen de [32, 3, 64, 64] (lote, canales, alto, ancho) se convierte en [32, 12288] (lote, total de píxeles), lo que prepara los datos para las capas lineales del MLP.\n","\n","**F.relu(layer(x)):** Aquí es donde se aplican la mayoría de los cálculos. layer(x) realiza el cálculo lineal, y F.relu aplica la función de activación ReLU. Las funciones de activación añaden no-linealidad, permitiendo al modelo aprender patrones complejos.\n","\n","**self.layers[-1](x):** La última capa no tiene una función de activación porque la función de pérdida (CrossEntropyLoss) que usarás más adelante ya se encarga de aplicar la función de activación final (softmax)."],"metadata":{"id":"t4rrst6cOxMS"}},{"cell_type":"markdown","source":["# **3. Fase de entrenamiento**\n","\n","Ahora que tenemos la estructura del MLP (MLP class) y los datos listos en DataLoaders, el siguiente paso es la fase de entrenamiento. Aquí es donde el modelo \"aprende\" a clasificar las imágenes.\n","\n","Este proceso implica tres elementos clave:\n","\n","1.   Función de Pérdida (Loss Function): Mide qué tan mal está el modelo en sus predicciones. Si la predicción es muy diferente de la etiqueta real, la pérdida es alta. Si es similar, la pérdida es baja.\n","\n","2.   Optimizador (Optimizer): Es el algoritmo que usa la pérdida para ajustar los pesos del modelo. Su objetivo es encontrar los mejores pesos para que la pérdida sea lo más baja posible.\n","\n","3.   Bucle de Entrenamiento (Training Loop): Es un ciclo que repite el proceso de pasar los datos por la red, calcular la pérdida y ajustar los pesos. Esto se hace por un número determinado de épocas."],"metadata":{"id":"FwrLZ1e1PoUz"}},{"cell_type":"markdown","source":["## **3.1 Configuración del entrenamiento**\n","Primero, necesitamos crear instancias del modelo, la función de pérdida y el optimizador.\n","*   **nn.CrossEntropyLoss():** Esta función de pérdida es perfecta para tu problema. Combina la función Softmax (que convierte los resultados del modelo en probabilidades) y la pérdida de entropía cruzada.\n","\n","*   **torch.optim.Adam:** Es uno de los optimizadores más eficientes y fáciles de usar. Es el motor que ajustará los pesos de tu red."],"metadata":{"id":"0pjyeiOXQYH9"}},{"cell_type":"code","source":["# Definimos los parámetros del modelo y del entrenamiento\n","num_clases = len(dataset_train.clases)\n","input_size = 3 * 64 * 64\n","hidden_layers = [128, 64]\n","lr = 0.0005\n","\n","# Cremaos una instancia del modelo\n","model = MLP(input_size=input_size, hidden_layers=hidden_layers, num_classes=num_clases)\n","\n","# Definimos la función de pérdida., CrossEntropyLoss es la función estándar para la clasificación multiclase\n","criterion = nn.CrossEntropyLoss()\n","\n","# Definimos el optimizador Adam es un optimizador popular y eficiente.\n","# El 'lr' (learning rate) controla el tamaño de los ajustes que hace el optimizador\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","# Movemos el modelo a la GPU si en caso este está disponible\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","print(f\"Modelo y optimizador configurados en el dispositivo: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FVswn2dNO-JJ","executionInfo":{"status":"ok","timestamp":1755829236139,"user_tz":300,"elapsed":8,"user":{"displayName":"Piero Andrés Díaz Carpio","userId":"17486678244017445526"}},"outputId":"91e2a354-ff3c-4aae-ca3b-cf67975a8eaf"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Modelo y optimizador configurados en el dispositivo: cpu\n"]}]},{"cell_type":"markdown","source":["## **3.2 Bucle de Entrenamiento y Validación**\n","Ahora, vamos a crear el bucle que hará que el modelo aprenda. Esto se realiza en dos etapas por cada época:\n","\n","*   **Entrenamiento**: El modelo ve los datos de entrenamiento y ajusta sus pesos.\n","\n","*   **Validación**: El modelo ve los datos de validación para evaluar su rendimiento en datos que no ha visto. Esto es clave para evitar el sobreajuste (overfitting).\n","\n","**Análisis del Bucle:**\n","\n","*   **optimizer.zero_grad():** Borra los gradientes de la iteración anterior para evitar que se acumulen.\n","\n","*   **loss.backward():** Calcula los gradientes de la pérdida con respecto a todos los parámetros del modelo. Esto es el \"autograd\" en acción.\n","\n","*   **optimizer.step():** Usa los gradientes calculados para actualizar los pesos del modelo.\n","\n","Este es el proceso completo de entrenamiento. Al final, se tendrá los modelos entrenados y los datos para graficar la pérdida y la precisión a lo largo de las épocas."],"metadata":{"id":"XP-mO9VbQx45"}},{"cell_type":"code","source":["# Definimos el de veces que el modelo verá todo el dataset, osea las épocas\n","num_epochs = 100\n","\n","# Creamos las listas para guardar las métricas de entrenamiento y validación\n","train_losses = []\n","val_losses = []\n","train_accuracies = []\n","val_accuracies = []\n","\n","for epoch in range(num_epochs):\n","    # Definimos el bucle de Entrenamiento\n","    model.train() # Configuramos el modelo en modo de entrenamiento\n","    running_loss = 0.0\n","    correct_train = 0\n","    total_train = 0\n","\n","    for images, labels in dataloader_train:\n","        images, labels = images.to(device), labels.to(device)\n","        # Reseteamos a cero los gradientes acumulados\n","        optimizer.zero_grad()\n","        # Pasamos los datos por la red\n","        outputs = model(images)\n","        # Calculamos la pérdida\n","        loss = criterion(outputs, labels)\n","        # Calculamos los gradientes (Paso hacia atrás)\n","        loss.backward()\n","        # Ajustamos los pesos del modelo\n","        optimizer.step()\n","        running_loss += loss.item() * images.size(0)\n","        # Calculamos la precisión\n","        _, predicted = torch.max(outputs.data, 1)\n","        total_train += labels.size(0)\n","        correct_train += (predicted == labels).sum().item()\n","\n","    avg_train_loss = running_loss / len(dataloader_train.dataset)\n","    train_accuracy = 100 * correct_train / total_train\n","\n","    # Definimos el bucle de Validación\n","    model.eval() # Configuramos el modelo en modo de evaluación\n","    running_val_loss = 0.0\n","    correct_val = 0\n","    total_val = 0\n","\n","    with torch.no_grad():\n","    # Deshabilitamos el cálculo de gradientes\n","        for images, labels in dataloader_val:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            loss_val = criterion(outputs, labels)\n","            running_val_loss += loss_val.item() * images.size(0)\n","\n","            _, predicted_val = torch.max(outputs.data, 1)\n","            total_val += labels.size(0)\n","            correct_val += (predicted_val == labels).sum().item()\n","\n","    avg_val_loss = running_val_loss / len(dataloader_val.dataset)\n","    val_accuracy = 100 * correct_val / total_val\n","\n","    # Guardamos las métricas\n","    train_losses.append(avg_train_loss)\n","    val_losses.append(avg_val_loss)\n","    train_accuracies.append(train_accuracy)\n","    val_accuracies.append(val_accuracy)\n","\n","    # Imprimimos el progreso\n","    print(f\"Época [{epoch+1}/{num_epochs}], Pérdida de Entrenamiento: {avg_train_loss:.4f}, Precisión de Entrenamiento: {train_accuracy:.2f}%, Pérdida de Validación: {avg_val_loss:.4f}, Precisión de Validación: {val_accuracy:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uNHbnom-Q74u","outputId":"2b902484-530a-4e78-94a1-e6c29bbb87b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Época [1/100], Pérdida de Entrenamiento: 2.2930, Precisión de Entrenamiento: 19.70%, Pérdida de Validación: 2.1686, Precisión de Validación: 22.89%\n","Época [2/100], Pérdida de Entrenamiento: 2.1277, Precisión de Entrenamiento: 24.81%, Pérdida de Validación: 2.1388, Precisión de Validación: 21.69%\n","Época [3/100], Pérdida de Entrenamiento: 2.0431, Precisión de Entrenamiento: 28.87%, Pérdida de Validación: 1.9464, Precisión de Validación: 32.53%\n","Época [4/100], Pérdida de Entrenamiento: 1.9521, Precisión de Entrenamiento: 31.73%, Pérdida de Validación: 1.9223, Precisión de Validación: 32.53%\n","Época [5/100], Pérdida de Entrenamiento: 1.8876, Precisión de Entrenamiento: 33.23%, Pérdida de Validación: 1.7793, Precisión de Validación: 34.94%\n","Época [6/100], Pérdida de Entrenamiento: 1.7732, Precisión de Entrenamiento: 40.30%, Pérdida de Validación: 1.8234, Precisión de Validación: 28.92%\n","Época [7/100], Pérdida de Entrenamiento: 1.7469, Precisión de Entrenamiento: 38.05%, Pérdida de Validación: 1.7247, Precisión de Validación: 34.94%\n","Época [8/100], Pérdida de Entrenamiento: 1.6927, Precisión de Entrenamiento: 41.80%, Pérdida de Validación: 1.6970, Precisión de Validación: 39.76%\n","Época [9/100], Pérdida de Entrenamiento: 1.6412, Precisión de Entrenamiento: 43.76%, Pérdida de Validación: 1.6483, Precisión de Validación: 44.58%\n","Época [10/100], Pérdida de Entrenamiento: 1.5824, Precisión de Entrenamiento: 46.02%, Pérdida de Validación: 1.6336, Precisión de Validación: 42.17%\n","Época [11/100], Pérdida de Entrenamiento: 1.5442, Precisión de Entrenamiento: 48.57%, Pérdida de Validación: 1.5573, Precisión de Validación: 44.58%\n","Época [12/100], Pérdida de Entrenamiento: 1.4967, Precisión de Entrenamiento: 48.87%, Pérdida de Validación: 1.5626, Precisión de Validación: 43.37%\n","Época [13/100], Pérdida de Entrenamiento: 1.5571, Precisión de Entrenamiento: 44.96%, Pérdida de Validación: 1.5498, Precisión de Validación: 40.96%\n","Época [14/100], Pérdida de Entrenamiento: 1.4497, Precisión de Entrenamiento: 50.53%, Pérdida de Validación: 1.4650, Precisión de Validación: 48.19%\n","Época [15/100], Pérdida de Entrenamiento: 1.4077, Precisión de Entrenamiento: 50.53%, Pérdida de Validación: 1.4477, Precisión de Validación: 42.17%\n","Época [16/100], Pérdida de Entrenamiento: 1.3711, Precisión de Entrenamiento: 53.68%, Pérdida de Validación: 1.4150, Precisión de Validación: 46.99%\n","Época [17/100], Pérdida de Entrenamiento: 1.3679, Precisión de Entrenamiento: 52.48%, Pérdida de Validación: 1.3723, Precisión de Validación: 51.81%\n","Época [18/100], Pérdida de Entrenamiento: 1.3825, Precisión de Entrenamiento: 51.28%, Pérdida de Validación: 1.5784, Precisión de Validación: 45.78%\n","Época [19/100], Pérdida de Entrenamiento: 1.3442, Precisión de Entrenamiento: 52.48%, Pérdida de Validación: 1.3560, Precisión de Validación: 48.19%\n","Época [20/100], Pérdida de Entrenamiento: 1.3218, Precisión de Entrenamiento: 53.53%, Pérdida de Validación: 1.4350, Precisión de Validación: 40.96%\n","Época [21/100], Pérdida de Entrenamiento: 1.2789, Precisión de Entrenamiento: 55.94%, Pérdida de Validación: 1.3779, Precisión de Validación: 50.60%\n","Época [22/100], Pérdida de Entrenamiento: 1.2425, Precisión de Entrenamiento: 58.95%, Pérdida de Validación: 1.4200, Precisión de Validación: 46.99%\n","Época [23/100], Pérdida de Entrenamiento: 1.2432, Precisión de Entrenamiento: 58.05%, Pérdida de Validación: 1.3051, Precisión de Validación: 54.22%\n","Época [24/100], Pérdida de Entrenamiento: 1.2286, Precisión de Entrenamiento: 58.95%, Pérdida de Validación: 1.3699, Precisión de Validación: 49.40%\n","Época [25/100], Pérdida de Entrenamiento: 1.2054, Precisión de Entrenamiento: 57.74%, Pérdida de Validación: 1.3377, Precisión de Validación: 48.19%\n","Época [26/100], Pérdida de Entrenamiento: 1.1956, Precisión de Entrenamiento: 59.70%, Pérdida de Validación: 1.4157, Precisión de Validación: 54.22%\n","Época [27/100], Pérdida de Entrenamiento: 1.2389, Precisión de Entrenamiento: 57.29%, Pérdida de Validación: 1.3303, Precisión de Validación: 54.22%\n","Época [28/100], Pérdida de Entrenamiento: 1.1587, Precisión de Entrenamiento: 58.95%, Pérdida de Validación: 1.3737, Precisión de Validación: 51.81%\n","Época [29/100], Pérdida de Entrenamiento: 1.1366, Precisión de Entrenamiento: 60.90%, Pérdida de Validación: 1.3319, Precisión de Validación: 49.40%\n","Época [30/100], Pérdida de Entrenamiento: 1.1259, Precisión de Entrenamiento: 59.40%, Pérdida de Validación: 1.3135, Precisión de Validación: 49.40%\n","Época [31/100], Pérdida de Entrenamiento: 1.0868, Precisión de Entrenamiento: 63.46%, Pérdida de Validación: 1.3429, Precisión de Validación: 53.01%\n","Época [32/100], Pérdida de Entrenamiento: 1.1314, Precisión de Entrenamiento: 61.05%, Pérdida de Validación: 1.3652, Precisión de Validación: 51.81%\n","Época [33/100], Pérdida de Entrenamiento: 1.0799, Precisión de Entrenamiento: 62.41%, Pérdida de Validación: 1.3014, Precisión de Validación: 54.22%\n","Época [34/100], Pérdida de Entrenamiento: 1.0808, Precisión de Entrenamiento: 64.51%, Pérdida de Validación: 1.3884, Precisión de Validación: 48.19%\n","Época [35/100], Pérdida de Entrenamiento: 1.0635, Precisión de Entrenamiento: 63.31%, Pérdida de Validación: 1.3480, Precisión de Validación: 51.81%\n","Época [36/100], Pérdida de Entrenamiento: 1.0290, Precisión de Entrenamiento: 63.76%, Pérdida de Validación: 1.3499, Precisión de Validación: 51.81%\n","Época [37/100], Pérdida de Entrenamiento: 1.0744, Precisión de Entrenamiento: 63.16%, Pérdida de Validación: 1.4108, Precisión de Validación: 53.01%\n","Época [38/100], Pérdida de Entrenamiento: 1.0200, Precisión de Entrenamiento: 66.47%, Pérdida de Validación: 1.3308, Precisión de Validación: 50.60%\n","Época [39/100], Pérdida de Entrenamiento: 1.0062, Precisión de Entrenamiento: 66.62%, Pérdida de Validación: 1.3819, Precisión de Validación: 53.01%\n","Época [40/100], Pérdida de Entrenamiento: 1.0181, Precisión de Entrenamiento: 64.96%, Pérdida de Validación: 1.3169, Precisión de Validación: 55.42%\n","Época [41/100], Pérdida de Entrenamiento: 1.0292, Precisión de Entrenamiento: 63.91%, Pérdida de Validación: 1.3240, Precisión de Validación: 54.22%\n","Época [42/100], Pérdida de Entrenamiento: 1.0850, Precisión de Entrenamiento: 61.50%, Pérdida de Validación: 1.4055, Precisión de Validación: 54.22%\n","Época [43/100], Pérdida de Entrenamiento: 0.9854, Precisión de Entrenamiento: 67.37%, Pérdida de Validación: 1.3325, Precisión de Validación: 53.01%\n","Época [44/100], Pérdida de Entrenamiento: 0.9987, Precisión de Entrenamiento: 66.47%, Pérdida de Validación: 1.4026, Precisión de Validación: 55.42%\n","Época [45/100], Pérdida de Entrenamiento: 0.9808, Precisión de Entrenamiento: 66.77%, Pérdida de Validación: 1.3252, Precisión de Validación: 56.63%\n","Época [46/100], Pérdida de Entrenamiento: 0.9540, Precisión de Entrenamiento: 65.56%, Pérdida de Validación: 1.3876, Precisión de Validación: 53.01%\n","Época [47/100], Pérdida de Entrenamiento: 0.9075, Precisión de Entrenamiento: 67.67%, Pérdida de Validación: 1.3154, Precisión de Validación: 51.81%\n","Época [48/100], Pérdida de Entrenamiento: 0.9318, Precisión de Entrenamiento: 67.67%, Pérdida de Validación: 1.3899, Precisión de Validación: 54.22%\n","Época [49/100], Pérdida de Entrenamiento: 0.9141, Precisión de Entrenamiento: 69.02%, Pérdida de Validación: 1.4096, Precisión de Validación: 57.83%\n","Época [50/100], Pérdida de Entrenamiento: 0.9042, Precisión de Entrenamiento: 70.08%, Pérdida de Validación: 1.3970, Precisión de Validación: 55.42%\n","Época [51/100], Pérdida de Entrenamiento: 0.9292, Precisión de Entrenamiento: 67.07%, Pérdida de Validación: 1.4088, Precisión de Validación: 56.63%\n","Época [52/100], Pérdida de Entrenamiento: 0.8997, Precisión de Entrenamiento: 70.08%, Pérdida de Validación: 1.3662, Precisión de Validación: 55.42%\n","Época [53/100], Pérdida de Entrenamiento: 0.8315, Precisión de Entrenamiento: 71.73%, Pérdida de Validación: 1.4018, Precisión de Validación: 56.63%\n"]}]},{"cell_type":"code","source":["# Encontramos la época y el valor de la precisión de validación máxima\n","best_epoch_index_val = np.argmax(val_accuracies)\n","best_epoch_val = best_epoch_index_val + 1\n","max_val_accuracy = val_accuracies[best_epoch_index_val]\n","\n","# Encontramos la época y el valor de la precisión del entrenamiento máximo\n","best_epoch_index_train = np.argmax(train_accuracies)\n","best_epoch_train = best_epoch_index_train + 1\n","max_train_accuracy = train_accuracies[best_epoch_index_train]\n","\n","# Encontramos la precisión de entrenamiento en la mejor validación\n","train_accuracy_at_best_epoch_val = train_accuracies[best_epoch_index_val]\n","val_accuracy_at_best_epoch_train = val_accuracies[best_epoch_index_train]\n","\n","# Imprimimos\n","print(f\"Época de máx. validación: {best_epoch_val}\")\n","print(f\"Máx. validación (%): {max_val_accuracy:.1f}\")\n","print(f\"Entrenamiento en máx. validación (%): {train_accuracy_at_best_epoch_val:.1f}\")\n","print(f\"Época de máx. entrenamiento: {best_epoch_train}\")\n","print(f\"Máx. entrenamiento (%): {max_train_accuracy:.1f}\")\n","print(f\"Validación en máx. entrenamiento (%): {val_accuracy_at_best_epoch_train:.1f}\")\n","\n","# Creamos el gráfico\n","plt.figure(figsize=(10, 6))\n","plt.plot(range(1, num_epochs + 1), train_accuracies, color='red', label='Precisión de Entrenamiento')\n","plt.plot(range(1, num_epochs + 1), val_accuracies, color='blue', label='Precisión de Validación')\n","plt.title('EXP07: Evolución de Precisión de Entrenamiento vs. Validación', fontsize=12)\n","plt.xlabel('Épocas', fontsize=12)\n","plt.ylabel('Precisión (%)', fontsize=12)\n","plt.grid(True)\n","plt.legend()\n","\n","# Marcamos el punto de la máxima precisión de validación (mejor época)\n","plt.plot(best_epoch_val, max_val_accuracy, 'bo', markersize=10)\n","\n","# Marcamos el punto de la precisión de entrenamiento en la mejor época de entrenavalidación\n","plt.plot(best_epoch_val, train_accuracy_at_best_epoch_val, 'ro', markersize=10)\n","\n","# Marcamos el punto de la máxima precisión de entrenamiento\n","plt.plot(best_epoch_train, max_train_accuracy, 'ro', markersize=10)\n","\n","# Marcamos el punto de la precisión de validación en la mejor época de entrenamiento\n","plt.plot(best_epoch_train, val_accuracy_at_best_epoch_train, 'bo', markersize=10)\n","\n","# Agregamos la etiqueta de datos para el punto de validación máximo.\n","plt.annotate(\n","    f'Mejor Época de Validación: {best_epoch_val}\\nMáx. Validación: {max_val_accuracy:.2f}%',\n","    xy=(best_epoch_val, max_val_accuracy),\n","    xytext=(10, 0),\n","    textcoords='offset points',\n",")\n","# Agregamos la etiqueta de datos para el punto de entrenamiento.\n","plt.annotate(\n","    f'Mejor Época de Validación: {best_epoch_val}\\nEntrenamiento: {train_accuracy_at_best_epoch_val:.2f}%',\n","    xy=(best_epoch_val, train_accuracy_at_best_epoch_val),\n","    xytext=(-130, 10),\n","    textcoords='offset points',\n",")\n","\n","# Agregamos la etiqueta de datos para el punto de entrenamiento máximo.\n","plt.annotate(\n","    f'Mejor Época de Entrenamiento: {best_epoch_train}\\nMáx. Entrenamiento: {max_train_accuracy:.2f}%',\n","    xy=(best_epoch_train, max_train_accuracy),\n","    xytext=(10, -30),\n","    textcoords='offset points',\n",")\n","# Agregamos la etiqueta de datos para el punto de validación.\n","plt.annotate(\n","    f'Mejor Época de Entrenamiento: {best_epoch_train}\\nValidación: {val_accuracy_at_best_epoch_train:.2f}%',\n","    xy=(best_epoch_train, val_accuracy_at_best_epoch_train),\n","    xytext=(10, 0),\n","    textcoords='offset points',\n",")\n","\n","plt.show()"],"metadata":{"id":"NoeyBS1dffB1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **4. Fase de evaluación del modelo**\n","Una vez que el modelo ha terminado de entrenar (con las épocas suficientes), el último paso es evaluarlo usando el conjunto de prueba. A diferencia de los conjuntos de entrenamiento y validación, el modelo nunca ha visto estos datos, por lo que la evaluación final nos dará una idea aterrizada de su rendimiento en el mundo real.\n","\n","Para esto, calcularemos las métricas siguientes: la matriz de confusión, el precision, el recall y el F1-score."],"metadata":{"id":"Lmw3RkaOS6BP"}},{"cell_type":"code","source":["# Asegúramos de que el modelo esté en modo de evaluación\n","model.eval()\n","\n","# Listamos para almacenar las predicciones y las etiquetas reales\n","y_true = []\n","y_pred = []\n","\n","# Desactivamos el cálculo de gradientes para la evaluación\n","with torch.no_grad():\n","    for images, labels in dataloader_test:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","\n","        _, predicted = torch.max(outputs.data, 1)\n","\n","        y_true.extend(labels.cpu().numpy())\n","        y_pred.extend(predicted.cpu().numpy())\n","\n","# Calculamos las métricas de precisión, recall y F1-score\n","precision, recall, f1_score, _ = precision_recall_fscore_support(y_true, y_pred, average=None)\n","\n","# Generamos la matriz de confusión\n","cm = confusion_matrix(y_true, y_pred)"],"metadata":{"id":"C5AvmT7WTS-_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **4.1 Visualización (limpio)**"],"metadata":{"id":"d_ORr5-JTbD7"}},{"cell_type":"code","source":["# Mapeamos los índices a los nombres de las clases (IDs)\n","clases_dict = {i: clase for i, clase in enumerate(dataset_test.clases)}\n","clases_a_mostrar = [clases_dict[i] for i in range(len(clases_dict))]\n","\n","# Obtenemos los conteos de imágenes y los convierte a un diccionario para fácil acceso\n","conteo_clases = df_imagenes['label'].value_counts().to_dict()\n","\n","# Ordenamos el diccionario de conteos según el orden de las clases a mostrar, para que los datos estén alineados correctamente\n","conteo_ordenado = [conteo_clases.get(clase, 0) for clase in clases_a_mostrar]\n","\n","# Creamos el diccionario de datos con todas las columnas\n","reporte_data = {\n","    'Clase': clases_a_mostrar,\n","    'Precisión': [f\"{p:.2f}\" for p in precision],\n","    'Recall': [f\"{r:.2f}\" for r in recall],\n","    'F1-score': [f\"{f:.2f}\" for f in f1_score],\n","    'Conteo': conteo_ordenado\n","}\n","\n","# Creamos el DataFrame final a partir de este único diccionario\n","df_reporte_final = pd.DataFrame(reporte_data)\n","\n","# Imprimimos el reporte de clasificación actualizado\n","print(\"Reporte de Clasificación con Conteo:\\n\")\n","print(df_reporte_final)\n","\n","# Para visualizar la matriz de confusión\n","plt.figure(figsize=(5, 5))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=clases_a_mostrar, yticklabels=clases_a_mostrar)\n","plt.xlabel('Clase Predicha')\n","plt.ylabel('Clase Verdadera')\n","plt.title('Matriz de Confusión')\n","plt.show()"],"metadata":{"id":"VF_RsrCsH8h-"},"execution_count":null,"outputs":[]}]}